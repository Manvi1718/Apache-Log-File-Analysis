{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manvi1718/Apache-Log-File-Analysis/blob/main/AI_assignmnet_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI ALGORITHMS"
      ],
      "metadata": {
        "id": "zQWWT8-XpQgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Required Libraries"
      ],
      "metadata": {
        "id": "Ho367G1fUOvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics import r2_score\n",
        "from gensim.models import Word2Vec\n",
        "import os\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from urllib.request import urlretrieve"
      ],
      "metadata": {
        "id": "ZSQeqeXPUNn4"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Device Agnostic Code"
      ],
      "metadata": {
        "id": "KQAK4tm5VFXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Wug6VtWqUi7d",
        "outputId": "727bf72c-22d3-4e86-e519-233080f30c95"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Linear Regression"
      ],
      "metadata": {
        "id": "qk1wWfXnb64_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression\n",
        "def linear_regression():\n",
        "    # Toy dataset\n",
        "    X, y = datasets.make_regression(n_samples=1000, n_features=1, noise=0.1)\n",
        "\n",
        "    # Convert to PyTorch tensors and move to GPU\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Linear Regression model\n",
        "    model = nn.Linear(1, 1).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Training\n",
        "    for epoch in range(200):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_tensor)\n",
        "        # Squeeze the output tensor to match the target tensor dimensions\n",
        "        outputs = outputs.squeeze(dim=1)\n",
        "        loss = criterion(outputs, y_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (epoch+1) % 20== 0:\n",
        "            print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
        "\n",
        "            # Calculate R-squared\n",
        "            with torch.no_grad():\n",
        "                predicted = model(X_tensor).cpu().numpy()\n",
        "                r2 = r2_score(y, predicted)\n",
        "                print(f'R-squared: {r2:.4f}')"
      ],
      "metadata": {
        "id": "5Q-TTgIVVIRS"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run linear regression\n",
        "linear_regression()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE7ugmTlXWu4",
        "outputId": "e37aad16-8cad-4f26-85aa-fd00f35f3bab"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/100], Loss: 79.0598\n",
            "R-squared: 0.5498\n",
            "Epoch [40/100], Loss: 34.8749\n",
            "R-squared: 0.8014\n",
            "Epoch [60/100], Loss: 15.3966\n",
            "R-squared: 0.9123\n",
            "Epoch [80/100], Loss: 6.8045\n",
            "R-squared: 0.9612\n",
            "Epoch [100/100], Loss: 3.0121\n",
            "R-squared: 0.9828\n",
            "Epoch [120/100], Loss: 1.3372\n",
            "R-squared: 0.9924\n",
            "Epoch [140/100], Loss: 0.5970\n",
            "R-squared: 0.9966\n",
            "Epoch [160/100], Loss: 0.2696\n",
            "R-squared: 0.9985\n",
            "Epoch [180/100], Loss: 0.1248\n",
            "R-squared: 0.9993\n",
            "Epoch [200/100], Loss: 0.0607\n",
            "R-squared: 0.9997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Value of R squared close to 1 ,means its a good fit"
      ],
      "metadata": {
        "id": "jB-HD3bhYuTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Logistic Regression"
      ],
      "metadata": {
        "id": "2nnnCqL_b_Q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "def logistic_regression(reg_strength):\n",
        "    # Toy dataset\n",
        "    iris = datasets.load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "    # train test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "    # Convert to PyTorch tensors and move to GPU\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Logistic Regression model\n",
        "    model = LogisticRegression(max_iter=100,C=reg_strength).fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)*100\n",
        "    print(f'Logistic Regression Accuracy: {accuracy:.2f}%')\n",
        "    print(f'Regularization Strength (C): {reg_strength}')\n"
      ],
      "metadata": {
        "id": "y3detYtFYyJR"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run logistic regression\n",
        "logistic_regression(0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgwSiHkQaY2Y",
        "outputId": "a2a28502-b066-4bed-bb64-8b05f2f19f19"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 80.00%\n",
            "Regularization Strength (C): 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularisation strength to reduce overfitting in logistic regression. use smaller value of C"
      ],
      "metadata": {
        "id": "tCFNyRaqbuJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Decision Tree"
      ],
      "metadata": {
        "id": "SheVNZ22cHK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Methods in Decision tree to solve overfitting isues in Decision Trees\n",
        "\n",
        "* Pruning - reduce the size of decision by removing its brances, done by `ccp_alpha` , it is the complexity parameter.\n",
        "\n",
        "* Limiting depth - limiting the maximum depth of the tree , done by setting `max_depth` parameter\n",
        "* Minimum samples split and leaf -\n",
        "we can also control the minimum number of samples to split `min_samples_split` and internal node and minimum number of samples required to be at leaf node `min_samples_leaf`."
      ],
      "metadata": {
        "id": "fUzHp0edccam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree\n",
        "def decision_tree():\n",
        "    # Toy dataset\n",
        "    iris = datasets.load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "    # Decision Tree model\n",
        "    model = DecisionTreeClassifier(ccp_alpha=0.01,max_depth=2)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)*100\n",
        "    print(f'Decision Tree Accuracy: {accuracy:.2f}%')\n"
      ],
      "metadata": {
        "id": "JS2j55YnayOK"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision_tree()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zehwNR4jcOsh",
        "outputId": "4a94bb87-8f02-4669-a4ac-0e0192e41709"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 90.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) SVM"
      ],
      "metadata": {
        "id": "IhNPXCVQfOSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def svm():\n",
        "    # Toy dataset\n",
        "    iris = datasets.load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "    # SVM model\n",
        "    model = SVC(C=0.1)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)*100\n",
        "    print(f'SVM Accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "rciRyljJcSCF"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNSDGutqeYhL",
        "outputId": "46f573d4-1eae-4096-e36d-98ef89518a5c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 90.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Naive bayes"
      ],
      "metadata": {
        "id": "rvQvp9qGfQMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_bayes():\n",
        "    # Toy dataset\n",
        "    newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "    X, y = newsgroups.data, newsgroups.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Naive-Bayes model\n",
        "    model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)*100\n",
        "    print(f'Naive-Bayes Accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "teFHH-odecxd"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_eFnBZFg6Sh",
        "outputId": "2ca61cfe-cc2c-4a8a-e067-6e2badfcbf8f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive-Bayes Accuracy: 65.78%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) TF-IDF"
      ],
      "metadata": {
        "id": "3OE50Bftg58Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "def tfidf():\n",
        "    # Toy dataset\n",
        "    newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "    X, y = newsgroups.data, newsgroups.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "    # TF-IDF model with reduced vocabulary size\n",
        "    vectorizer = TfidfVectorizer(max_features=1000,ngram_range=(1,2))  # Adjust max_features as needed\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "    X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "    # Convert to PyTorch tensors and move to GPU\n",
        "    X_train_tensor = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32).to(device)\n",
        "    X_test_tensor = torch.tensor(X_test_tfidf.toarray(), dtype=torch.float32).to(device)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "\n",
        "    # Simple Neural Network model with reduced hidden size\n",
        "    class SimpleNN(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size, output_size):\n",
        "            super(SimpleNN, self).__init__()\n",
        "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "            self.relu = nn.ReLU()\n",
        "            self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        def forward(self, x):\n",
        "            out = self.fc1(x)\n",
        "            out = self.relu(out)\n",
        "            out = self.fc2(out)\n",
        "            return out\n",
        "\n",
        "    # Instantiate the model with reduced hidden size\n",
        "    model = SimpleNN(input_size=X_train_tensor.shape[1], hidden_size=32, output_size=len(set(y))).to(device)\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Training\n",
        "    for epoch in range(50):  # Reduced number of epochs\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train_tensor)\n",
        "        loss = criterion(outputs, y_train_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/50], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Testing\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_test_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        accuracy = accuracy_score(y_test, predicted.cpu().numpy())*100\n",
        "\n",
        "    print(f'TF-IDF Neural Network Accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "GFX3icxLfjLx"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zgBVRGihJSj",
        "outputId": "327fcc13-e374-494b-8ec5-96c6587649e4"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/50], Loss: 2.9897\n",
            "Epoch [20/50], Loss: 2.9661\n",
            "Epoch [30/50], Loss: 2.9358\n",
            "Epoch [40/50], Loss: 2.8986\n",
            "Epoch [50/50], Loss: 2.8541\n",
            "TF-IDF Neural Network Accuracy: 24.69%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Word2Vec"
      ],
      "metadata": {
        "id": "b-bluQmRmPX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th-ozDBLmN3C",
        "outputId": "4d195843-87d0-42e2-c172-a1d778f289fa"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec\n",
        "def word2vec():\n",
        "    # Download the 'punkt' resource\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "\n",
        "    # Toy dataset\n",
        "    corpus = [\n",
        "        \"This is the first document.\",\n",
        "        \"This document is the second document.\",\n",
        "        \"And this is the third one.\",\n",
        "        \"Is this the first document?\",\n",
        "    ]\n",
        "\n",
        "    # Tokenize and train Word2Vec model\n",
        "    tokenized_corpus = [nltk.word_tokenize(sentence.lower()) for sentence in corpus]\n",
        "    word2vec_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "    # Example: Finding the vector for a word\n",
        "    vector = word2vec_model.wv['document']\n",
        "    print(\"Word2Vec Vector for 'document':\")\n",
        "    print(vector)"
      ],
      "metadata": {
        "id": "EYUrWLL9fjIO"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNN00W8ThPGx",
        "outputId": "13816ebb-7bd3-45b7-ea33-aef38b7605c0"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec Vector for 'document':\n",
            "[-5.3761393e-04  2.3459077e-04  5.1012170e-03  9.0115219e-03\n",
            " -9.3035055e-03 -7.1186870e-03  6.4577162e-03  8.9744031e-03\n",
            " -5.0161965e-03 -3.7644049e-03  7.3809391e-03 -1.5342169e-03\n",
            " -4.5370674e-03  6.5543531e-03 -4.8609949e-03 -1.8136933e-03\n",
            "  2.8776617e-03  9.8915887e-04 -8.2834894e-03 -9.4506554e-03\n",
            "  7.3119737e-03  5.0714435e-03  6.7562792e-03  7.6230383e-04\n",
            "  6.3530928e-03 -3.4065295e-03 -9.4848091e-04  5.7711215e-03\n",
            " -7.5222286e-03 -3.9373739e-03 -7.5092558e-03 -9.2885981e-04\n",
            "  9.5392875e-03 -7.3166536e-03 -2.3360765e-03 -1.9363161e-03\n",
            "  8.0779977e-03 -5.9297686e-03  4.5617318e-05 -4.7524953e-03\n",
            " -9.6023204e-03  5.0089518e-03 -8.7604597e-03 -4.3930719e-03\n",
            " -3.5214103e-05 -2.9548592e-04 -7.6621324e-03  9.6163880e-03\n",
            "  4.9832016e-03  9.2352722e-03 -8.1572160e-03  4.4980138e-03\n",
            " -4.1374546e-03  8.2675234e-04  8.4996261e-03 -4.4643688e-03\n",
            "  4.5164214e-03 -6.7876368e-03 -3.5471660e-03  9.3982853e-03\n",
            " -1.5784105e-03  3.2352045e-04 -4.1381051e-03 -7.6831253e-03\n",
            " -1.5093960e-03  2.4685122e-03 -8.8934030e-04  5.5310265e-03\n",
            " -2.7425813e-03  2.2589052e-03  5.4565049e-03  8.3474834e-03\n",
            " -1.4548642e-03 -9.2107793e-03  4.3709916e-03  5.6996895e-04\n",
            "  7.4426048e-03 -8.1429747e-04 -2.6364601e-03 -8.7528294e-03\n",
            " -8.5558271e-04  2.8258313e-03  5.4009468e-03  7.0547434e-03\n",
            " -5.7027498e-03  1.8572190e-03  6.0867225e-03 -4.7976980e-03\n",
            " -3.1054933e-03  6.7991368e-03  1.6290357e-03  1.9226066e-04\n",
            "  3.4747359e-03  2.1982045e-04  9.6214656e-03  5.0585950e-03\n",
            " -8.9198640e-03 -7.0399828e-03  9.0400706e-04  6.3935039e-03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8) GloVe"
      ],
      "metadata": {
        "id": "2-mW-Qq0nG74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_glove_model(output_file):\n",
        "    glove_url = 'https://nlp.stanford.edu/data/glove.6B.zip'\n",
        "    glove_zip_file = 'glove.6B.zip'\n",
        "\n",
        "    # Download GloVe zip file\n",
        "    urlretrieve(glove_url, glove_zip_file)\n",
        "\n",
        "    # Unzip GloVe file\n",
        "    from zipfile import ZipFile\n",
        "    with ZipFile(glove_zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "    # Convert GloVe format to Word2Vec format\n",
        "    glove_txt_file = 'glove.6B.100d.txt'\n",
        "    glove2word2vec(glove_txt_file, output_file)\n",
        "\n",
        "    # Clean up - remove downloaded zip file\n",
        "    os.remove(glove_zip_file)\n",
        "\n",
        "    return output_file"
      ],
      "metadata": {
        "id": "pzMIDJ0snxDH"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def glove():\n",
        "    # Define the output file for the converted GloVe model\n",
        "    word2vec_output_file = 'glove.6B.100d.word2vec'\n",
        "\n",
        "    # Download and convert GloVe model if not already downloaded\n",
        "    if not os.path.exists(word2vec_output_file):\n",
        "        download_glove_model(word2vec_output_file)\n",
        "\n",
        "    # Load GloVe embeddings using Gensim\n",
        "    glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
        "\n",
        "    # Example: Finding the vector for a word\n",
        "    vector = glove_model['document']\n",
        "    print(\"Glove Vector for 'document':\")\n",
        "    print(vector)"
      ],
      "metadata": {
        "id": "xr-Vnjw9n6uN"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcFTfbFuoECk",
        "outputId": "ad834a89-bab5-47a0-8da1-5b4c4d06f4a8"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Glove Vector for 'document':\n",
            "[-2.7285e-01 -9.6449e-02  4.1131e-01  3.7925e-01  8.9352e-01  4.5227e-01\n",
            "  1.9478e-01 -3.6985e-01  5.9704e-01  1.3387e-01  4.2878e-01 -2.8012e-01\n",
            "  2.0141e-01 -1.9995e-02 -6.2983e-02  7.1399e-01  8.9025e-01 -3.1009e-01\n",
            " -1.9911e-01 -4.6591e-01 -8.8145e-01 -5.4318e-01 -5.2839e-01  7.0794e-02\n",
            " -3.1042e-01 -9.8677e-01  1.0283e-01  1.6911e-01 -4.4878e-01  1.6171e-01\n",
            "  3.9394e-01  1.2655e-01 -1.2540e-01 -6.6462e-02 -1.2977e-01 -3.9406e-02\n",
            "  4.4811e-02 -4.2534e-01  2.6742e-02 -3.8609e-01 -8.4547e-01 -6.4412e-02\n",
            "  6.8974e-01  2.4521e-01 -7.3434e-01 -7.7389e-01 -1.5336e-01 -2.9057e-01\n",
            " -6.8358e-01 -3.8785e-01  1.2230e+00  1.7723e-01  1.6004e-01  8.3723e-01\n",
            " -3.1238e-01 -1.3138e+00 -2.6000e-01 -4.8754e-01  1.6751e+00  1.7320e-01\n",
            " -2.9494e-01  1.6038e-01 -5.3087e-01 -9.0950e-01  6.7436e-01 -5.2625e-01\n",
            " -3.0406e-01  8.5552e-01 -2.6879e-01 -9.0492e-01  3.0380e-01  2.0591e-01\n",
            "  3.3439e-01 -6.2308e-01  6.4306e-02  2.2179e-01 -9.2076e-02  2.1894e-01\n",
            " -1.4015e+00 -4.4588e-02  2.6263e-01  1.5343e-01 -8.8158e-04 -2.2226e-02\n",
            " -1.3228e+00 -6.3649e-02  9.7797e-01 -8.1209e-01  1.5083e-02  2.4391e-01\n",
            " -1.9343e-01 -1.7141e-01 -1.1954e-01  1.3623e-01  3.4787e-01 -5.0286e-02\n",
            " -1.8547e-01 -7.1763e-01  1.0898e-01  1.1472e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9) Lemmatization"
      ],
      "metadata": {
        "id": "QteYNjL_oLSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization - NLP\n",
        "def lemmatization():\n",
        "    import nltk\n",
        "\n",
        "    # Download WordNet\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "    # Toy dataset\n",
        "    corpus = [\n",
        "        \"This is the first document.\",\n",
        "        \"This document is the second document.\",\n",
        "        \"And this is the third one.\",\n",
        "        \"Is this the first document?\",\n",
        "    ]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_corpus = [' '.join([lemmatizer.lemmatize(word) for word in nltk.word_tokenize(sentence)]) for sentence in corpus]\n",
        "\n",
        "    print(\"Lemmatized Corpus:\")\n",
        "    print(lemmatized_corpus)"
      ],
      "metadata": {
        "id": "5cDnkenTfjCq"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbvBcRqWocy2",
        "outputId": "2843241e-9b87-489b-80ef-0a2f9a2ae259"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Corpus:\n",
            "['This is the first document .', 'This document is the second document .', 'And this is the third one .', 'Is this the first document ?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10) Recurrent Neural Network (RNN)"
      ],
      "metadata": {
        "id": "QiJTMHAUpDiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Toy dataset\n",
        "sequences = [\n",
        "    \"this is good\",\n",
        "    \"that is bad\",\n",
        "    \"this is excellent\",\n",
        "    \"that is horrible\"\n",
        "]\n",
        "labels = [1, 0, 1, 0]\n",
        "\n",
        "# Tokenize and convert to PyTorch tensors\n",
        "tokenized_sequences = [nltk.word_tokenize(sentence.lower()) for sentence in sequences]\n",
        "word2vec_model = Word2Vec(sentences=tokenized_sequences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "X_rnn = torch.tensor([word2vec_model.wv[s] for s in tokenized_sequences], dtype=torch.float32).to(device)\n",
        "y_rnn = torch.tensor(labels, dtype=torch.float32).to(device)\n"
      ],
      "metadata": {
        "id": "oD85CQeZr8-S"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import movie_reviews\n",
        "import random\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Download the movie reviews dataset from nltk\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "# Load the movie reviews and labels\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# Shuffle the documents to mix positive and negative reviews\n",
        "random.shuffle(documents)\n",
        "\n",
        "# Separate the reviews and labels\n",
        "reviews = [\" \".join(words) for words, label in documents]\n",
        "labels = [1 if label == 'pos' else 0 for words, label in documents]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize and convert to PyTorch tensors\n",
        "def tokenize_and_vectorize(data, max_length=200):\n",
        "    tokenized_data = [word_tokenize(sentence.lower()) for sentence in data]\n",
        "    vectorized_data = torch.zeros(len(tokenized_data), max_length, 100)\n",
        "\n",
        "    for i, sentence in enumerate(tokenized_data):\n",
        "        for j, word in enumerate(sentence):\n",
        "            if j == max_length:\n",
        "                break\n",
        "            if word in word2vec_model.wv:\n",
        "                vectorized_data[i, j, :] = torch.tensor(word2vec_model.wv[word])\n",
        "\n",
        "    return vectorized_data\n",
        "\n",
        "X_train_tensor = tokenize_and_vectorize(X_train)\n",
        "X_test_tensor = tokenize_and_vectorize(X_test)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Simple RNN model with non-linearity\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.activation = nn.Tanh()  # Tanh activation\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.activation(out)  #Tanh activation\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Instantiate the RNN model\n",
        "rnn_model = SimpleRNN(input_size=100, hidden_size=64, output_size=1)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(rnn_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = rnn_model(X_train_tensor)\n",
        "    loss = criterion(outputs.squeeze(), y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        rnn_model.eval()\n",
        "        predictions = (torch.sigmoid(rnn_model(X_test_tensor)) > 0.5).float()\n",
        "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predictions.cpu().numpy()) * 100\n",
        "        print(f'Epoch [{epoch + 1}/10], Loss: {loss.item():.4f}, Accuracy: {accuracy:.2f}%')\n",
        "        rnn_model.train()  # Set back to training mode\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPPysIsoqP-I",
        "outputId": "ce7eed2a-a795-4a82-f552-a35aced265b2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6932, Accuracy: 50.25%\n",
            "Epoch [2/10], Loss: 0.6933, Accuracy: 50.25%\n",
            "Epoch [3/10], Loss: 0.6931, Accuracy: 49.75%\n",
            "Epoch [4/10], Loss: 0.6931, Accuracy: 49.75%\n",
            "Epoch [5/10], Loss: 0.6932, Accuracy: 49.75%\n",
            "Epoch [6/10], Loss: 0.6931, Accuracy: 50.25%\n",
            "Epoch [7/10], Loss: 0.6931, Accuracy: 49.50%\n",
            "Epoch [8/10], Loss: 0.6931, Accuracy: 50.25%\n",
            "Epoch [9/10], Loss: 0.6931, Accuracy: 50.25%\n",
            "Epoch [10/10], Loss: 0.6931, Accuracy: 50.25%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVWuMCC46z1yII7heJx8ml",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}